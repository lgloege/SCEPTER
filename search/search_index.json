{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"SCEPTER","text":"<p>SCEPTER (Soil Cycles of Elements simulator for Predicting TERrestrial regulation of greenhouse gases) is a reactive transport model that simulates the reactions and transport of solid, aqueous, and gas species within soil, including the dissolution/precipitation of minerals, three-phase biogeochemical reaction, bio-mixing and uplift/erosion of solid phases, advective and diffusive transport of aqueous species, and gaseous diffusion.</p> <p>The model is developed for simulating natural and enhanced weathering processes, with its specific features that allow for:</p> <ul> <li>the explicit bio-mixing of soil, including tilling by farmers</li> <li>addition of solid materials on the topsoil</li> <li>tracking of particle size distributions, which facilitates surface area calculations for individual solid species.</li> </ul>"},{"location":"#running-scepter","title":"Running SCEPTER","text":"<p>This documentation describes how to setup SCEPTER and run new simulations. We encourage new users to start with the Example Simulations, which reproduces output from Kanzaki et al. (2024).</p>"},{"location":"#citing-scepter","title":"Citing SCEPTER","text":"<p>If you use SCEPTER in your publications, we encourage you to cite the following manuscript:</p> <p> Kanzaki, Y., Chiaravalloti, I., Zhang, S., Planavsky, N. J., &amp; Reinhard, C. T. (2024).  In silico calculation of soil pH by SCEPTER v1.0.  Geoscientific Model Development, 17(10), 4515-4532,  https://doi.org/10.5194/gmd-17-4515-2024. </p>"},{"location":"example-sims/","title":"Example Simulations","text":"<p>Here I will walk through the steps to run simulations from Kanzaki et al. (2024). These simulations are described in section 3 of the manuscript</p>"},{"location":"example-sims/#test-for-sikora-buffer","title":"Test for Sikora buffer","text":"<ol> <li>in-silico field samples <ol> <li>modify outdir in L20 of <code>spinup_inert.py</code></li> <li>type: <code>python3 spinup_inert.py</code></li> </ol> </li> <li>buffer pH for field samples in silico<ol> <li>modify outdir in L528 and undo comments-out in L849-852 of <code>get_soilpH_time.py</code></li> <li>type: <code>python3 get_soilpH_time.py</code></li> </ol> </li> </ol>"},{"location":"example-sims/#cation-exchange-experiment","title":"Cation exchange experiment","text":"<ol> <li>cation exchange equilibrium simulation<ol> <li>modify outdir in L176 of <code>test_phreeqc_ex11_init.py</code></li> <li>type: <code>python3 test_phreeqc_ex11_init.py</code></li> </ol> </li> <li>cation exchange + advection + dispersion simulation<ol> <li>spin-up<ol> <li>modify outdir in L176 of <code>test_phreeqc_ex11.py</code></li> <li>type: <code>python3 test_phreeqc_ex11.py</code></li> </ol> </li> <li>dynamic simulation <ol> <li>undo comment-out in L16,17 of <code>makefile</code></li> <li>type: <code>make</code> </li> <li>undo comment-out in L277-280 of <code>test_phreeqc_ex11.py</code></li> <li>type: <code>python3 test_phreeqc_ex11.py</code></li> </ol> </li> </ol> </li> </ol>"},{"location":"example-sims/#mesocosm-experiment","title":"Mesocosm experiment","text":"<ol> <li> <p>field simulation</p> <ol> <li>modify outdir in L159 of <code>spinup_inrt2.py</code></li> <li>type: <code>python3 spinup_inrt2.py</code></li> </ol> </li> <li> <p>laboratory simulation</p> <ol> <li>modify outdir in L528, L888 of <code>get_soilpH_time.py</code> </li> <li>type: <code>python3 get_soilpH_time.py</code></li> </ol> </li> </ol>"},{"location":"example-sims/#alkalinity-requiremnt-for-erw","title":"Alkalinity requiremnt for ERW","text":"<ol> <li> <p>spin/tune-up </p> <ol> <li>modify outdir in L148 of <code>tunespin_3_newton_inert_buff_v2_clean.py</code></li> <li>type: <code>python3 tunespin_3_newton_inert_buff_v2_clean.py spinup_run_name 21.103289732688683 6.058006742238197 20.980309042371502 2.0516666666666667 8.222189843654622 0.282726679550165 0.35136107875550837 0.0010131311683626316 1.005952418781816</code></li> </ol> <p>Note</p> <p>The runtime inputs are to specify: run ID, CEC (cmol/kg), target soil pH,  target exchange acidity (%CEC), target soil OM (wt%), temperature (\u00b0C), moisture, runoff (m/yr), erosion rate (m/yr), nitrification rate (gN/m<sup>2</sup>/yr) </p> </li> <li> <p>basalt application </p> <ol> <li>modify outdir in L93 of basalt_buff_tunespin_bisec_v2.py and     option of using soil/porewater pH (phnorm_pw=False/True, L25-26)</li> <li>type: <code>python3 basalt_buff_tunespin_bisec_v2.py 6.2 1 21.103289732688683 basalt_run_name spinup_run_name</code></li> </ol> <p>Note</p> <p>The runtime inputs are to specify:  target pH, duration of run, CEC (cmol/kg), basalt run name, spin/tune-up run name </p> </li> <li> <p>calculating soil pH prodiles</p> <ol> <li>modify outdir in L49, L299 of <code>get_soilpH_time_dep.py</code></li> <li>type: <code>get_soilpH_time_dep.py basalt_run_name</code></li> </ol> </li> </ol>"},{"location":"hpc-guidance/","title":"Yale Grace Cluster","text":"<p>Yale Grace cluster is a shared high performance computer (HPC) maintained by the Yale Center for Research and Computing (YCRC). The YCRC maintains an excellent documentation page, but I will provide an abbreviated version on this page. However, for more details on any topic please see the YCRC documentation.</p>"},{"location":"hpc-guidance/#steps-to-get-setup-and-log-into-grace","title":"Steps to Get Setup and Log Into Grace","text":"<ol> <li>Request an account if you do not already have one</li> <li>Send us your public SSH key with the YCRC's SSH key uploader. Allow up to ten minutes for it to propagate. See the box below if you need help finding or generating an SSH key. SSH is the Secure SHell protocol and it allows you to connect to remote (or cloud) computers, such as Grace. </li> <li>Once YCRC has your public key then you can connect to Grace with the following terminal command: <pre><code>ssh netid@clustername.ycrc.yale.edu\n</code></pre> If you are off campus you will need to first connect to Yale's VPN</li> </ol> <p>Creating an SSH Key</p> <p>First see if you have any SSH keys on your computer by copying the following command into the terminal </p> <pre><code>ls ~/.ssh/*.pub\n</code></pre> <p>If you anything is displayed, then you have you have an SSH key. If you do, then you use this command to copy the key (the name may be different depending on the encryption algorithm used):  <pre><code>pbcopy &lt; ~/.ssh/id_ed25519.pub\n</code></pre></p> <p>Alternatively, you can just print the key to the screen and then copy it:</p> <pre><code>more ~/.ssh/id_ed25519.pub\n</code></pre> <p>Finally, if you do not have a key, then use the following command to generate one (make sure to change the email address):</p> <pre><code>ssh-keygen -t ed25519 -C \"your_email@example.com\"\n</code></pre>"},{"location":"hpc-guidance/#creating-a-python-environment-on-grace","title":"Creating a Python Environment on Grace","text":"<p>Use the following steps to create a conda Python environment on Grace. These steps are an abbreviated version of YCRC's more detailed instructions.</p> <ol> <li>Log into grace with the <code>ssh netid@clustername.ycrc.yale.edu</code></li> <li>Start an interactice session of the devel partion by copying the following command into the terminal (need to be on Grace):  <pre><code>salloc --partition=devel --mem=15G --time=2:00:00 --cpus-per-task=2\n</code></pre> This puts you into a compute node for two hours and gives you 15G of memory</li> <li>Now we need to load miniconda. This can done with the following command: <pre><code>module load miniconda\n</code></pre></li> <li>Once miniconda is loaded, you can create a new environment like this: <pre><code>conda create -n scepter python numpy pandas matplotlib jupyter jupyterlab\n</code></pre> Here I named the environment scepter and installed relevant packages into it. We only really need <code>numpy</code>, but the others may be useful for analysis later</li> <li>Now let's load the environment into OOD so we can use in a Jupyter notebook --- if you decide to JupyterLab. First, reset the modules: <pre><code>module reset\n</code></pre> Then run the following command to load it into the OOD: <pre><code>ycrc_conda_env.sh update\n</code></pre></li> <li>That's it! Now you have a conda environment and can use it a Jupyter notebook.</li> </ol>"},{"location":"hpc-guidance/#yale-grace-cluster_1","title":"Yale GRACE cluster","text":"<p>The following SLURM script can be used to submit a job to the cluster. Make sure to modify the SLURM directives to match your account name and resources requirements. Consult the Grace documentation page for more information on the different partitions and their compute nodes and job limits. YCRC also provides guidance on running jobs with SLURM scripts. </p> <pre><code>#!/bin/bash\n\n#SBATCH --job-name=scepter\n#SBATCH --ntasks=1\n#SBATCH --time=10:00\n#SBATCH --account eisaman\n#SBATCH --nodes 1\n#SBATCH --mem 1G\n#SBATCH --partition devel\n\nmodule load netCDF-Fortran/4.6.0-iompi-2022b\nmodule load OpenBLAS/0.3.27-GCC-13.3.0\n\nmake\n</code></pre> <p>In the code above, the lines with <code>#SBATCH</code> are called SLURM directives. These instructions tell the SLURM job scheduler how to manage and allocate resources for your job on the HPC. The directives in the code above do the following:</p> <ul> <li><code>#SBATCH --job-name=scepter</code>: Sets the name of the job to \"scepter\", which is helpful for identifying it in job queues and logs.</li> <li><code>#SBATCH --ntasks=1</code>: Requests 1 task for the job. This is often used for serial jobs.</li> <li><code>#SBATCH --time=10:00</code>: Sets the maximum run time for the job to 10 minutes. After this time, the job will be killed if not completed.</li> <li><code>#SBATCH --account=eisaman</code>: Charges the job's resource usage to the \"eisaman\" account. </li> <li><code>#SBATCH --nodes=1</code>: Requests 1 compute node for the job.</li> <li><code>#SBATCH --mem=1G</code>: Requests 1 gigabyte of memory for the job.</li> <li><code>#SBATCH --partition=devel</code>: Submits the job to the \"devel\" partition.</li> </ul> <p>See the table here for descriptions of other directives.</p> <p>Note</p> <p>Each user can only have 1 job running in the <code>devel</code> partition. Devel is typically used for short or development jobs. This is also where open-on-demand jobs run on Grace. So, if you are unable to submit to this parititon it likely means you already have something running there. Maybe try the <code>day</code> partition.</p>"},{"location":"hpc-guidance/#submit-your-job-and-check-the-status","title":"submit your job and check the status","text":"<p>Once you have your submit script create you can run it with the following command:</p> <pre><code>sbatch submit.sbatch\n</code></pre> <p>This assumes you named your script <code>submit.sbatch</code>. The name can you whatever you want. If you want to be clear that this a shell script you could call it <code>submit.sh</code>.</p> <p>To check the status of all your jobs in the queue you can use:</p> <pre><code>squeue --me\n</code></pre> <p>The latter will list the status of the job and it will also list a job ID. This job ID can be used to cancel the job if you need to. Let's say the job ID is 1234, you can use the following command to cancel the job:</p> <pre><code>scancel 1234\n</code></pre> <p>Finally you can check the effeciency of the job with:</p> <pre><code>seff 1234\n</code></pre>"}]}